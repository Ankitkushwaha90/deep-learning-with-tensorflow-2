{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pkbar ## tracing losses during training\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    def __init__(self, tensor):\n",
    "        self.weights = tensor\n",
    "        self.gradients = np.zeros_like(self.weights)\n",
    "        self.bias = np.zeros((tensor.shape[-1]))\n",
    "        self.bias_gradients = np.zeros_like(self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.parameters = None\n",
    "    def init_param(self, tensor):\n",
    "        param = Parameter(tensor)\n",
    "        self.parameters = param\n",
    "        return param\n",
    "    def update(self, optimizer):\n",
    "        optimizer.update(self.parameters)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475af0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    '''\n",
    "    #### Objective\n",
    "    A class which defines the linear layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "        tensor = np.random.randn(inputs, outputs)\n",
    "        self.parameters = self.init_param(tensor)\n",
    "    def backward(self, D, X):\n",
    "        '''\n",
    "        ###### Objective\n",
    "        A backward pass\n",
    "        ######  Input\n",
    "        partial derivative with respect to end features and start features\n",
    "        ##### Output\n",
    "        partial derivative with respect to start function\n",
    "        '''\n",
    "        return D@self.parameters.weights.T### D*theta.T\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        ###### Objective\n",
    "        A forward pass\n",
    "        ######  Input\n",
    "        start features\n",
    "        ##### Output\n",
    "        end features\n",
    "        '''\n",
    "        return X@self.parameters.weights + self.parameters.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.parameters = None\n",
    "    def backward(self, D, X):\n",
    "        S = 1/(1+np.exp(-X))\n",
    "        return D*(S*(1-S))\n",
    "    def forward(self, X):\n",
    "        return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.parameters = None\n",
    "    \n",
    "    def backward(self, D, X):\n",
    "        return D\n",
    "    def softmax(self, X):\n",
    "        k = np.sum(np.exp(X), axis = 1)\n",
    "        t = 0\n",
    "        X_out = []\n",
    "        \n",
    "        for i in X:\n",
    "            X_out.append(list(np.exp(i)/k[t]))\n",
    "            t+=1\n",
    "        return np.array(X_out)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc897db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "    def update(self, param):\n",
    "        '''\n",
    "        ##### Objective\n",
    "        Update of parameters using gradient descent\n",
    "        ##### Input\n",
    "        Parameters\n",
    "        ##### Output\n",
    "        Updated Parameters\n",
    "        '''\n",
    "        param.weights -= self.lr*param.gradients\n",
    "        param.bias -= self.lr *param.bias_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ba57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        self.parameters = None\n",
    "    def backward(self, D, X):\n",
    "        return D*(1-(np.tanh(X)*np.tanh(X)))\n",
    "    def forward(self, X):\n",
    "        return np.tanh(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f89bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.parameters = None\n",
    "    def backward(self, D, X):\n",
    "        return D*(1*(X>0))\n",
    "    def forward(self, X):\n",
    "        return np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model()\n",
    "    def __init__(self):\n",
    "        self.computational_graph = []\n",
    "    def add(self, layer):\n",
    "        '''\n",
    "        ###### Objective\n",
    "        Add a layer to the computational graph\n",
    "        ###### Input\n",
    "        layer\n",
    "        \n",
    "        '''\n",
    "        self.computational_graph.append(layer)\n",
    "    def compiler(self, loss, optimizer):\n",
    "        '''\n",
    "        ###### Objective \n",
    "        compile a model(give it all additional properties needed for training)\n",
    "        ###### Input\n",
    "        loss and optimizer to be used\n",
    "        '''\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        ###### Objective \n",
    "        A forward pass\n",
    "        ###### Input\n",
    "        Input data for model\n",
    "        ###### Output\n",
    "        predicted data by model, plus intermediary layers\n",
    "        '''\n",
    "        Y_int = X\n",
    "        Y_int_list = []\n",
    "        \n",
    "        for layer in self.computational_graph:\n",
    "            Y_int_list.append(Y_int)\n",
    "            Y_ = layer.forward(Y_int)\n",
    "            Y_int = Y_\n",
    "        return Y_, Y_int_list\n",
    "    def fit_batch(self, X,Y):\n",
    "        '''\n",
    "        ####### OBjective\n",
    "        optimize the parameters of a particular batch\n",
    "        ###### Input \n",
    "        the input and output of dataset\n",
    "        \n",
    "        ##### Ouput\n",
    "        loss\n",
    "        \n",
    "        '''\n",
    "        out = X\n",
    "        Y_, Y_int_list = self.forward(X)\n",
    "        \n",
    "        D = predicted_output - target_output\n",
    "        L,D = self.loss(Y_,Y)\n",
    "        for Y_int, layer in zip(Y_int_list[::-1], self.computational_graph[::-1]):\n",
    "            D = layer.backward(D,Y_int)\n",
    "            if layer.parameters is not None:\n",
    "                layer.update(self.optimizer)\n",
    "        return L\n",
    "    def fit(self, X,Y, epochs, bs):\n",
    "        losses = []\n",
    "        \n",
    "        pbar = pkbar.Pbar(name='Training', target = epochs)\n",
    "        kbar = pkbar.Kbar(target=epochs)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss = 0.0\n",
    "            for i in range(0, len(X), bs):\n",
    "                loss += self.fit_batch(X[i:i+bs], Y[i:i+bs])\n",
    "            losses.append(loss)\n",
    "            kbar.update(epoch+1, values=[('loss',loss)])\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]], dtype = float)\n",
    "Y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cfb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "model = Model()\n",
    "model.add(Linear(2,10))\n",
    "model.add(Sigmoid())\n",
    "model.add(Linear(10,1))\n",
    "model.add(Sigmoid())\n",
    "\n",
    "model.compiler(cce_loss, SGD(lr=5e-1))\n",
    "losses = model.fit(X,Y, epochs = EPOCHS, bs = X.shape[0])\n",
    "plt.plot(range(1,EPOCHS+1), losses)\n",
    "\n",
    "plt.ylabel('BCE')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945af1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8006b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cc910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8eb9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bc5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9052153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

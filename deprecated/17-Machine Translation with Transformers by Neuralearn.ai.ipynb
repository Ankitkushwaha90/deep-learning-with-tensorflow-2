{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import(SimpleRNN,Embedding,Input,LSTM,Input,\n",
    "                                    Dropout,Dense,GRU,LayerNormalization,\n",
    "                                    Bidirectional,Reshape)\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451711be",
   "metadata": {},
   "source": [
    "<H1>DATA PREPARATION</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES=250\n",
    "VALIDATION_RATIO=1\n",
    "VALIDATION_BRIDGE=int(VALIDATION_RATIO*NUM_EXAMPLES)\n",
    "\n",
    "text_dataset=tf.data.TextLineDataset(path).take(NUM_EXAMPLES)\n",
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844da811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(input_text):\n",
    "    return tf.strings.split(input_text,'\\t')[0:1],'starttoken'+tf.strings.split(input_text,'\\t')[1:2],tf.strings.split(input_text,'\\t')[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset=text_dataset.map(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(input_data):\n",
    "    '''\n",
    "    Task: Preprocess sentences or standardize the sentences\n",
    "    Input: raw reviews\n",
    "    output: standardized reviews\n",
    "    '''\n",
    "    output=tf.strings.lower(input_data)\n",
    "    outputs=tf.strings.regex_replace(output,\"<[^>]+>\",\"\")\n",
    "    outputs=tf.strings.regex_replace(output,\"<[%s]\"%re.esceape(string.punctuation),\" \")\n",
    "    outputs=tf.strings.regex_replace(output,\"  \",\" \")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d03098",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=10\n",
    "\n",
    "vectorize_input_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_pre_output_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_output_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:x)\n",
    "vectorize_input_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:y)\n",
    "vectorize_pre_output_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e21fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:z)\n",
    "vectorize_output_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b449d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_INPUT_SIZE=len(vectorize_input_layer.get_vocabulary())\n",
    "VOCAB_PRE_OUTPUT_SIZE=len(vectorize_pre_output_layer.get_vocabulary())\n",
    "VOCAB_OUTPUT_SIZE=len(vectorize_output_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x,y,z):\n",
    "    return {'in1':tf.squeeze(vectorize_input_layer(x),0),'in2':tf.squeeze(vectorize_pre_output_layer(y),0),}, tf.squeeze(vectorize_output_layer(z),0)\n",
    "dataset=text_dataset.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b012b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64007b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.shuffle(NUM_EXAMPLES)\n",
    "train_dataset=dataset.take(VALIDATION_BRIDGE)\n",
    "validation_dataset=dataset.skip(VALIDATION_BRIDGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset=validation_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a325f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_output_layer.get_vocabulary()[1372]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21332947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b789875",
   "metadata": {},
   "source": [
    "<H1>MODELING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.model_size=model_size\n",
    "    def call(self,query,key,value,sequence,look_ahead_masking=False):\n",
    "        #score=tf.matmul(query,key,transpose_b=True)\n",
    "        score=tf.einsum('ijk,ibk->ijb',query,key)\n",
    "        score/=tf.math.sqrt(tf.cast(self.model_size,tf.float32))\n",
    "        ones=tf.ones_like(score)\n",
    "        pad_mask=padding_mask(sequence)\n",
    "        \n",
    "        total_mask=pad_mask\n",
    "        if look_ahead_masking:\n",
    "            ahead_mask=1-tf.linalg.band_part(ones,-1,0)\n",
    "            total_mask+=ahead_mask\n",
    "        score+=total_mask*-1e10\n",
    "        alignment=tf.nn.softmax(score,axis=-1)\n",
    "        head=tf.matmul(alignment,value)\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(a):\n",
    "    return tf.expand_dims(tf.cast(tf.math.equal([a],0),tf.float32)[0],axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(model_size):\n",
    "    output=[]\n",
    "    for pos in range(SEQUENCE_LENGTH):\n",
    "        PE=np.zeros((model_size))\n",
    "        for i in range(model_size):\n",
    "            if i%2==0:\n",
    "                PE[i]=np.sin(pos/(10000**(i/model_size)))\n",
    "            else:\n",
    "                PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n",
    "        output.append(tf.expand_dims(PE,axis=0))\n",
    "    return tf.concat(output,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ac783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size,h):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.query_size=model_size//h\n",
    "        self.key_size=model_size//h\n",
    "        self.value_size=model_size//h\n",
    "        self.h=h\n",
    "        self.dense_q=[Dense(self.query_size) for _ in range(h)]\n",
    "        self.dense_k=[Dense(self.key_size) for _ in range(h)]\n",
    "        self.dense_v=[Dense(self.value_size) for _ in range(h)]\n",
    "        self.dense_o=Dense(model_size)\n",
    "        self.self_attention=SelfAttention(self,key_size)\n",
    "        \n",
    "    def call(self,query,key,value,sequence,look_ahead_masking):\n",
    "        heads=[]\n",
    "        \n",
    "        for i in range(self.h):\n",
    "            head=self.self_attention(self.dense_q[i](query),self.dense_k[i](key),\n",
    "                                    self.dense_v[i](value),sequence,look_ahead_masking)\n",
    "            heads.append(head)\n",
    "        heads=tf.concat(heads,axis=2)\n",
    "        heads=self.dense_o(heads)\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(vocab_size,model_size)\n",
    "        self.multi_attention=MultiHeadAttention(model_size,h)\n",
    "        self.dropout=Dropout(0.2)\n",
    "        \n",
    "        self.dense_1=Dense(model_size*4,activation='relu')\n",
    "        self.dense_2=Dense(model_size)\n",
    "        self.feed_forward_norm=LayerNormalization()\n",
    "        \n",
    "    def call(self,enc_in,sequence):\n",
    "        enc_out=self.multi_attention(enc_in,enc_in,enc_in,sequence,look_ahead_masking=False)\n",
    "        enc_out=enc_in+enc_out\n",
    "        enc_out=self.attention_norm(enc_out)\n",
    "        \n",
    "        feed_forward_in=enc_out\n",
    "        feed_forward_out=self.dropout(self.dense_2(self.dense_1(feed_forward_in)))\n",
    "        feed_forward_out+=feed_forward_in\n",
    "        feed_forward_out=self.feed_forward_norm(feed_forward_out)\n",
    "        return feed_forward_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c474bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h,num_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(vocab_size,model_size)\n",
    "        self.encoder_layer=[EncoderLayer(vocab_size,model_size,h) for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, sequence):\n",
    "        enc_in=self.embedding(sequence)\n",
    "        enc_in+=tf.cast(positional_embedding(self.model_size),dtype=tf.float32)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            out=self.encoder_layer[i](enc_in,sequence)\n",
    "            enc_in=out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acdf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size,num_layers,h):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        \n",
    "        self.multi_attention_bot=MultiHeadAttention(model_size,h)\n",
    "        self.attetnion_bot_norm=LayerNormalization()\n",
    "        \n",
    "        self.multi_attention_mid=MultiHeadAttention(model_size,h)\n",
    "        self.attetnion_mid_norm=LayerNormalization()\n",
    "        \n",
    "        self.dense_1=Dense(model_size*4,activation='relu')\n",
    "        self.dense_2=Dense(model_size)\n",
    "        self.dropout=Dropout(0.2)\n",
    "        \n",
    "        self.feed_forward_norm=LayerNormalization()\n",
    "        \n",
    "    def call(self,enc_in,sequence):\n",
    "        bot_dec_out=self.multi_attention_bot(bot_dec_in,bot_dec_in,bot_dec_in,sequence,look_ahead_masking=True)\n",
    "        bot_dec_out+=bot_dec_in\n",
    "        bot_dec_out=self.attention_bot_norm(bot_dec_out)\n",
    "        \n",
    "        mid_dec_in=bot_dec_out\n",
    "        \n",
    "        mid_dec_out=self.multi_attention_mid(mid_dec_in,mid_dec_in,mid_dec_in,sequence,look_ahead_masking=False)\n",
    "        mid_dec_out+=mid_dec_in\n",
    "        mid_dec_out=self.attention_mid_norm(mid_dec_out)\n",
    "        \n",
    "        feed_forward_in=mid_dec_out\n",
    "        \n",
    "        feed_forward_out=self.dropout(self.dense_2(self.dense_1(feed_forward_in)))\n",
    "        feed_forward_out+=feed_forward_in\n",
    "        feed_forward_out=self.feed_forward_norm(feed_forward_out)\n",
    "        return feed_forward_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h,num_layers):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(pre_vocab_size,model_size)\n",
    "        self.decoder_layer=[DecoderLayer(model_size,num_layers,h) for _ in range(num_layers)]\n",
    "        self.dense=Dense(vocab_size,)\n",
    "        \n",
    "    def call(self, sequence,encoder_output):\n",
    "        dec_in=self.embedding(sequence)\n",
    "        dec_in+=tf.cast(positional_embedding(self.model_size),dtype=tf.float32)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            out=self.decoder_layer[i](dec_in,encoder_output,sequence)\n",
    "            dec_in=out\n",
    "        out=self.dense(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,VOCAB_INPUT_SIZE,VOCAB_PRE_OUTPUT_SIZE,VOCAB_OUTPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.encoder=Encoder(\n",
    "            vocab_size=VOCAB_INPUT_SIZE,\n",
    "            model_size=MODEL_SIZE,\n",
    "            h=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "        )\n",
    "        \n",
    "        self.decoder=Decoder(\n",
    "            pre_vocab_size=VOCAB_PRE_OUTPUT_SIZE,\n",
    "            vocab_size=VOCAB_OUTPUT_SIZE,\n",
    "            model_size=MODEL_SIZE,\n",
    "            h=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs,pre_outputs):\n",
    "        x=self.encoder(inputs)\n",
    "        x=self.decoder(pre_outputs,x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=Input(SEQUENCE_LENGTH)\n",
    "pre_outputs=Input(SEQUENCE_LENGTH)\n",
    "\n",
    "transformer=Transformer(VOCAB_INPUT_SIZE,VOCAB_PRE_OUTPUT_SIZE,VOCAB_OUTPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)\n",
    "decoder_output=transformer(inputs,pre_outputs)\n",
    "model=tf.keras.Model([inputs,pre_outputs],decoder_output,name='transformer')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b4822",
   "metadata": {},
   "source": [
    "<H1>TRAINING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEU(tf.keras.metrics.Metric):\n",
    "    def __init__(self,name='bleu_score'):\n",
    "        super(BLEU,self).__init__()\n",
    "        self.add=0\n",
    "        self.total=0\n",
    "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
    "        y_true=tf.argmax(y_true,-1)\n",
    "        y_pred=tf.argmax(y_pred,-1)\n",
    "        \n",
    "        for i,j in zip(y_pred,y_true):\n",
    "            tf.autograph.experimental.set_loop_options()\n",
    "            self.total+=tf.math.count_nonzero(i)\n",
    "            for word in i:\n",
    "                if word==0:\n",
    "                    break\n",
    "                for q in range(len(j)):\n",
    "                    if j[q]==0:\n",
    "                        break\n",
    "                    if word==j[q]:\n",
    "                        self.add+=1\n",
    "                        j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n",
    "                        break\n",
    "    def result(self):\n",
    "        return self.add/self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "EPOCH=100\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=LR,),\n",
    "    #metrics=[BLEU()],\n",
    "    #run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='...'\n",
    "log_dir='...'\n",
    "callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646752a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35659bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_dataset, validation_data=validation_dataset,verbose=1,epochs=EPOCH,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927a112",
   "metadata": {},
   "source": [
    "<H1>TESTING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "    print('Input:-->',input_sentence)\n",
    "    test_data=tf.data.Dataset.from_tensor_slices([[input_sentence]])\n",
    "    input_test_data=test_data.map(vectorize_input_layer)\n",
    "    \n",
    "    for i in input_test_data.take(1):\n",
    "        in_1=i\n",
    "    in_2=[2]\n",
    "    final_output=[]\n",
    "    length=SEQUENCE_LENGTH\n",
    "    \n",
    "    for i in range(SEQUENCE_LENGTH):\n",
    "        p_in_2=tf.pad(tf.constant(in_2),[[0,SEQUENCE_LENGTH-1-I]])\n",
    "        output=tf.argmax(model.predict([[in_1],tf.expand_dims(p_in_2,0)]),-1)[0][i]\n",
    "        if output==0:\n",
    "            length=i\n",
    "            break\n",
    "        in_2.append(output.numpy()+1)\n",
    "        final_output.append(output.numpy())\n",
    "        \n",
    "    return [vectorize_output_layer.get_vocabulary()[i] for i in final_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('we won')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b009cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257e382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ae0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e3099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29368c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107a3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bfffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0694be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0804f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

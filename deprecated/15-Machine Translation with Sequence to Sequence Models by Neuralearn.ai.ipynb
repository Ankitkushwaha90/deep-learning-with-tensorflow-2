{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae647638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import SimpleRNN,Embedding,Input,LSTM,Dense,GRU,Bidirectional,Reshape\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a6fb5",
   "metadata": {},
   "source": [
    "<H1>DATA PREPARATION</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731503bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351faa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES=250\n",
    "VALIDATION_RATIO=1\n",
    "VALIDATION_BRIDGE=int(VALIDATION_RATIO*NUM_EXAMPLES)\n",
    "\n",
    "text_dataset=tf.data.TextLineDataset(path).take(NUM_EXAMPLES)\n",
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(input_text):\n",
    "    return tf.strings.split(input_text,'\\t')[0:1],'starttoken'+tf.strings.split(input_text,'\\t')[1:2],tf.strings.split(input_text,'\\t')[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467905bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset=text_dataset.map(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7dc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(input_data):\n",
    "    '''\n",
    "    Task: Preprocess sentences or standardize the sentences\n",
    "    Input: raw reviews\n",
    "    output: standardized reviews\n",
    "    '''\n",
    "    output=tf.strings.lower(input_data)\n",
    "    outputs=tf.strings.regex_replace(output,\"<[^>]+>\",\"\")\n",
    "    outputs=tf.strings.regex_replace(output,\"<[%s]\"%re.esceape(string.punctuation),\" \")\n",
    "    outputs=tf.strings.regex_replace(output,\"  \",\" \")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=10\n",
    "\n",
    "vectorize_input_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab25b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_pre_output_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52440346",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_output_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55903ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:x)\n",
    "vectorize_input_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa77275",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:y)\n",
    "vectorize_pre_output_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:z)\n",
    "vectorize_output_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc21601",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_INPUT_SIZE=len(vectorize_input_layer.get_vocabulary())\n",
    "VOCAB_PRE_OUTPUT_SIZE=len(vectorize_pre_output_layer.get_vocabulary())\n",
    "VOCAB_OUTPUT_SIZE=len(vectorize_output_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d49ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x,y,z):\n",
    "    return {'in1':tf.squeeze(vectorize_input_layer(x),0),'in2':tf.squeeze(vectorize_pre_output_layer(y),0),}, tf.squeeze(vectorize_output_layer(z),0)\n",
    "dataset=text_dataset.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed69815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_pre_output_layer.get_vocabulary()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.shuffle(NUM_EXAMPLES)\n",
    "train_dataset=dataset.take(VALIDATION_BRIDGE)\n",
    "validation_dataset=dataset.skip(VALIDATION_BRIDGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset=validation_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48420778",
   "metadata": {},
   "source": [
    "<H1>MODELING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0125b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=8\n",
    "LSTM_ENCODER_HIDDEN_SIZE=300\n",
    "LSTM_DECODER_HIDDEN_SIZE=1000\n",
    "SENTENCE_LENGTH=10\n",
    "\n",
    "inputs=Input(SEQUENCE_LENGTH)\n",
    "pre_out=Input(SEQUENCE_LENGTH)\n",
    "\n",
    "x = Embedding(\n",
    "    VOCAB_INPUT_SIZE,\n",
    "    EMBEDDING_DIM)(inputs)\n",
    "encoder = LSTM(\n",
    "    LSTM_HIDDEN_SIZE,\n",
    "    return_sequences=False,\n",
    "    return_state=True)\n",
    "\n",
    "_,h,c=encoder(x)\n",
    "x=Embedding(VOCAB_OUPUT_SIZE,EMBEDDING_DIM)(pre_out)\n",
    "decoder=LSTM(LSTM_DECODER_HIDDEN_SIZE,\n",
    "            return_state=True,\n",
    "            return_sequences=True)\n",
    "h=Dense(LSTM_DECODER_HIDDEN_SIZE)(h)\n",
    "c=Dense(LSTM_DECODER_HIDDEN_SIZE)(c)\n",
    "\n",
    "x,h,c=decoder(x,[h,c])\n",
    "x=Dense(VOCAB_OUTPUT_SIZE,activation='softmax')(x)\n",
    "model=tf.keras.Model([inputs,pre_out],x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d2d7a",
   "metadata": {},
   "source": [
    "<H1>TRAINING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEU(tf.keras.metrics.Metric):\n",
    "    def __init__(self,name='bleu_score'):\n",
    "        super(BLEU,self).__init__()\n",
    "        self.add=0\n",
    "        self.total=0\n",
    "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
    "        y_true=tf.argmax(y_true,-1)\n",
    "        y_pred=tf.argmax(y_pred,-1)\n",
    "        \n",
    "        for i,j in zip(y_pred,y_true):\n",
    "            tf.autograph.experimental.set_loop_options()\n",
    "            self.total+=tf.math.count_nonzero(i)\n",
    "            for word in i:\n",
    "                if word==0:\n",
    "                    break\n",
    "                for q in range(len(j)):\n",
    "                    if j[q]==0:\n",
    "                        break\n",
    "                    if word==j[q]:\n",
    "                        self.add+=1\n",
    "                        j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n",
    "                        break\n",
    "    def result(self):\n",
    "        return self.add/self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "EPOCH=100\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=LR,),\n",
    "    metrics=[BLEU()],\n",
    "    run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='...'\n",
    "log_dir='...'\n",
    "callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e453e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39975631",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_dataset, validation_data=validation_dataset,verbose=1,epochs=EPOCH,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca73f6e",
   "metadata": {},
   "source": [
    "<H1>TESTING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278acaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=tf.data.Dataset.from_tensor_slices([['i will try']])\n",
    "init_test_data=tf.data.Dataset.from_tensor_slices([['starttoken']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_data=test_data.map(vectorize_input_layer)\n",
    "pre_output_test_data=init_test_data.map(vectorize_pre_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7310c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in input_test_data.take(1):\n",
    "    print(i)\n",
    "    in_1=i\n",
    "for i in pre_output_test_data.take(1):\n",
    "    print(i)\n",
    "    in_2=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(in_1,in_2):\n",
    "    return tf.argmax(model.predict([in_1,in_2]),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=get_output(in_1,in_2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(SEQUENCE_LENGTH):\n",
    "    print(vectorize_output_layer.get_vocabulary()[output[0][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fb38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb584a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966d9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52104814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f117ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048c25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217809c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

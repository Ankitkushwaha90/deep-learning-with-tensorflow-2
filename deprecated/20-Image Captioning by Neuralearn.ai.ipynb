{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import(SimpleRNN,Embedding,Input,LSTM,Input,Conv1D,Softmax\n",
    "                                    Dropout,Dense,GRU,LayerNormalization,Reshape,\n",
    "                                    Bidirectional,Reshape)\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451711be",
   "metadata": {},
   "source": [
    "<H1>DATA PREPARATION</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c55e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN=10000\n",
    "INPUT_DIM=224\n",
    "BATCH_SIZE=8\n",
    "MODEL_SIZE=128\n",
    "SEQUENCE_LENGTH=20\n",
    "NUM_LAYERS=2\n",
    "NUM_HEADS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='...'\n",
    "txt_path='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(txt_path,\"r+\",encoding=\"utf-8\")\n",
    "lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}\n",
    "captions=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844da811",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines)):\n",
    "    split=lines[i].split('\\t')\n",
    "    data_dict[split[0]]=split[1][:-3]\n",
    "    captions+=split[1][:-3]\n",
    "captions+=3000*' starttoken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=MAX_TOKEN,\n",
    "                                               oov_token=\"<unk>\",\n",
    "                                               filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(captions.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21332947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e2e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,images,batch_size,tokenizers,data_dict,starttoken,INPUT_DIM):\n",
    "        self.images=images\n",
    "        self.batch_size=batch_size\n",
    "        self.train_image_list=os.listdir(images)\n",
    "        self.tokenizer=tokenizer\n",
    "        self.data_dict=data_dict\n",
    "        self.starttoken=starttoken\n",
    "        self.INPUT_DIM=INPUT_DIM\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.train_image_list)/self.batch_size))\n",
    "    def __getitem__(self,idx):\n",
    "        X,y=self.__data_generation(idx)\n",
    "        return X,y\n",
    "    def __data_generation(self,idx):\n",
    "        X=[]\n",
    "        y_1=[]\n",
    "        y_2=[]\n",
    "        start=tf.constant(self.batch_size*5*[[self.starttoken]])\n",
    "        \n",
    "        for j in range(idx*self.batch_size,(idx+1)*self.batch_size):\n",
    "            im_array=img_to_array(load_img(self.images+os.listdir(self.images)[j],target_size=(self.INPUT_DIM,self.INPUT_DIM)))\n",
    "            X=5*self.BATCH_SIZE*[im_array]\n",
    "            for i in range(5):\n",
    "                caption=self.data_dict[os.listdir(self.images)[j]+'#'+str(i)]\n",
    "                cap_seq=np.array(self.tokenizer.texts_to_sequences(caption.split(' '))).T\n",
    "                cap_tok=tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                    cap_seq,maxlen=20,padding='post',truncating='post')\n",
    "                y_1.append(cap_tok[0][:-1])\n",
    "                y_2.append(cap_tok[0])\n",
    "        X=tf.constant(X)\n",
    "        y_1=tf.constant(y_1)\n",
    "        y_2=tf.constant(y_2)\n",
    "        \n",
    "        y_1=tf.concat([start,y_1],axis=-1)\n",
    "        return {'in_1':X,'in_2':y_1},y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images='...'\n",
    "val_images='...'\n",
    "\n",
    "starttoken=tokenizer.word_index['starttoken']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65570e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen=DataGenerator(train_images,BATCH_SIZE,tokenizer,data_dict,starttoken,INPUT_DIM)\n",
    "val_gen=DataGenerator(val_images,BATCH_SIZE,tokenizer,data_dict,starttoken,INPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b789875",
   "metadata": {},
   "source": [
    "<H1>MODELING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed044d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.model_size=model_size\n",
    "    def call(self,query,key,value,sequence,look_ahead_masking=False):\n",
    "        #score=tf.matmul(query,key,transpose_b=True)\n",
    "        score=tf.einsum('ijk,ibk->ijb',query,key)\n",
    "        score/=tf.math.sqrt(tf.cast(self.model_size,tf.float32))\n",
    "        ones=tf.ones_like(score)\n",
    "        pad_mask=padding_mask(sequence)\n",
    "        \n",
    "        total_mask=pad_mask\n",
    "        if look_ahead_masking:\n",
    "            ahead_mask=1-tf.linalg.band_part(ones,-1,0)\n",
    "            total_mask+=ahead_mask\n",
    "        score+=total_mask*-1e10\n",
    "        alignment=tf.nn.softmax(score,axis=-1)\n",
    "        head=tf.matmul(alignment,value)\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c35420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(a):\n",
    "    return tf.expand_dims(tf.cast(tf.math.equal([a],0),tf.float32)[0],axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(model_size):\n",
    "    output=[]\n",
    "    for pos in range(SEQUENCE_LENGTH):\n",
    "        PE=np.zeros((model_size))\n",
    "        for i in range(model_size):\n",
    "            if i%2==0:\n",
    "                PE[i]=np.sin(pos/(10000**(i/model_size)))\n",
    "            else:\n",
    "                PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n",
    "        output.append(tf.expand_dims(PE,axis=0))\n",
    "    return tf.concat(output,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7774df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size,h):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.query_size=model_size//h\n",
    "        self.key_size=model_size//h\n",
    "        self.value_size=model_size//h\n",
    "        self.h=h\n",
    "        self.dense_q=[Dense(self.query_size) for _ in range(h)]\n",
    "        self.dense_k=[Dense(self.key_size) for _ in range(h)]\n",
    "        self.dense_v=[Dense(self.value_size) for _ in range(h)]\n",
    "        self.dense_o=Dense(model_size)\n",
    "        self.self_attention=SelfAttention(self,key_size)\n",
    "        \n",
    "    def call(self,query,key,value,sequence,look_ahead_masking):\n",
    "        heads=[]\n",
    "        \n",
    "        for i in range(self.h):\n",
    "            head=self.self_attention(self.dense_q[i](query),self.dense_k[i](key),\n",
    "                                    self.dense_v[i](value),sequence,look_ahead_masking)\n",
    "            heads.append(head)\n",
    "        heads=tf.concat(heads,axis=2)\n",
    "        heads=self.dense_o(heads)\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size,num_layers,h):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        \n",
    "        self.multi_attention_bot=MultiHeadAttention(model_size,h)\n",
    "        self.attetnion_bot_norm=LayerNormalization()\n",
    "        \n",
    "        self.multi_attention_mid=MultiHeadAttention(model_size,h)\n",
    "        self.attetnion_mid_norm=LayerNormalization()\n",
    "        \n",
    "        self.dense_1=Dense(model_size*4,activation='relu')\n",
    "        self.dense_2=Dense(model_size)\n",
    "        self.dropout=Dropout(0.2)\n",
    "        \n",
    "        self.feed_forward_norm=LayerNormalization()\n",
    "        \n",
    "    def call(self,enc_in,sequence):\n",
    "        bot_dec_out=self.multi_attention_bot(bot_dec_in,bot_dec_in,bot_dec_in,sequence,look_ahead_masking=True)\n",
    "        bot_dec_out+=bot_dec_in\n",
    "        bot_dec_out=self.attention_bot_norm(bot_dec_out)\n",
    "        \n",
    "        mid_dec_in=bot_dec_out\n",
    "        \n",
    "        mid_dec_out=self.multi_attention_mid(mid_dec_in,mid_dec_in,mid_dec_in,sequence,look_ahead_masking=False)\n",
    "        mid_dec_out+=mid_dec_in\n",
    "        mid_dec_out=self.attention_mid_norm(mid_dec_out)\n",
    "        \n",
    "        feed_forward_in=mid_dec_out\n",
    "        \n",
    "        feed_forward_out=self.dropout(self.dense_2(self.dense_1(feed_forward_in)))\n",
    "        feed_forward_out+=feed_forward_in\n",
    "        feed_forward_out=self.feed_forward_norm(feed_forward_out)\n",
    "        return feed_forward_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h,num_layers):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(pre_vocab_size,model_size)\n",
    "        self.decoder_layer=[DecoderLayer(model_size,num_layers,h) for _ in range(num_layers)]\n",
    "        self.dense=Dense(vocab_size,)\n",
    "        \n",
    "    def call(self, sequence,encoder_output):\n",
    "        dec_in=self.embedding(sequence)\n",
    "        dec_in+=tf.cast(positional_embedding(self.model_size),dtype=tf.float32)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            out=self.decoder_layer[i](dec_in,encoder_output,sequence)\n",
    "            dec_in=out\n",
    "        out=self.dense(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    base_model=ResNet50(weights='imagenet',input_shape=(INPUT_DIM,INPUT_DIM,3),include_top=False)\n",
    "    base_model.trainable=False\n",
    "    \n",
    "    conv4_block6_2_relu=[base_model.get_layer(layer_name).output for layer_name in [\"conv4_block6_2_relu\"]]\n",
    "    return tf.keras.Model(\n",
    "        inputs=[base_model.inputs],outputs=conv4_block6_2_relu)\n",
    "get_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=Input((INPUT_DIM,INPUT_DIM,3))\n",
    "pre_outputs=Input(SEQUENCE_LENGTH,)\n",
    "\n",
    "x = Rescaling(1/255.)(inputs)\n",
    "x=get_base_model()(inputs)\n",
    "x=Conv2D(256,3,padding='same',activation='relu')(x)\n",
    "x=Conv2D(128,3,padding='same',activation='relu')(x)\n",
    "x=BatchNormalization()(x)\n",
    "\n",
    "x=Conv2D(64,3,padding='same',activation='relu')(x)\n",
    "x=Conv2D(1,3,padding='same',activation='relu')(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Reshape((14,14))(x)\n",
    "\n",
    "x=Dense(MODEL_SIZE,activation='relu')(x)\n",
    "\n",
    "dec=Decoder(vocab_size=MAX_TOKEN,model_size=MODEL_SIZE,h=NUM_HEADS,num_layers=NUM_LAYERS)\n",
    "decoder_ouput=dec(pre_outputs,x)\n",
    "\n",
    "model=tf.keras.Model([inputs,pre_outputs],decoder_output,name='conv-transformer')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e26903",
   "metadata": {},
   "source": [
    "<H1>TRAINING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb21263",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "EPOCH=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=LR,),\n",
    "    metrics='accuracy',\n",
    "    run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8559844",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='...'\n",
    "callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac24b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_gen,verbose=1, shuffle=True,epochs=EPOCH,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef4899",
   "metadata": {},
   "source": [
    "<h1>TESTING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption(input_image):\n",
    "    plt.imshow(input_image[0]/255)\n",
    "    plt.show()\n",
    "    \n",
    "    in_1=input_image\n",
    "    in_2=[starttoken]\n",
    "    final_output=[]\n",
    "    length=SEQUENCE_LENGTH\n",
    "    \n",
    "    for i in range(SEQUENCE_LENGTH):\n",
    "        p_in_2=tf.pad(tf.constant(in_2),[[0,SEQUENCE_LENGTH-1-i]])\n",
    "        output=tf.argmax(model.predict([in_1,tf.expand_dims(p_in_2,0)]),-1)[0][i]\n",
    "        \n",
    "        if output==0:\n",
    "            length=1\n",
    "            break\n",
    "        in_2.append(output.numpy())\n",
    "        final_output.append(output.numpy())\n",
    "    print(final_output)\n",
    "    return ' '.join([tokenizer.index_word[i] for i in final_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im='...'\n",
    "test_image=val_images+im\n",
    "X=[]\n",
    "X.append(img_to_array(load_img(test_image,target_size=(224,224))))\n",
    "print(caption(tf.constant(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0694be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0804f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65e3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

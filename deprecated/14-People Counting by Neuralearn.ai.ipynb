{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf### models\n",
    "import numpy as np### math computations\n",
    "import matplotlib.pyplot as plt### plotting bar chart\n",
    "import sklearn### machine learning library\n",
    "import cv2## image processing\n",
    "import scipy.io\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
    "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
    "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
    "                                     RandomContrast, Rescaling, Resizing, Reshape,LeakyReLU)\n",
    "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
    "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
    "from tensorflow.keras.regularizers import L2, L1\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451711be",
   "metadata": {},
   "source": [
    "<H1>DATA PREPARATION</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c55e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_X,IN_Y=768,1024\n",
    "OUT_X,OUT_Y=96,128\n",
    "SUBSAMPLING_FACTOR=IN_X//OUT_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_distribution(x,u=0,sigma=10):\n",
    "    return np.expand_dims(1/(np.sqrt(2*np.pi*(sigma**2)))*np.exp(-(0.5)*(((x-u)/sigma)**2)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density_map_gaussian(im,points,gaussian_radius=4):\n",
    "    density_map=np.zeros((OUT_X,OUT_Y))\n",
    "    w,h=OUT_Y,OUT_X\n",
    "    num_gt=len(points)\n",
    "    \n",
    "    for point in points:\n",
    "        point=np.round(point).astype(int)\n",
    "        point[0],point[1]=min(h-1,point[1]),min(w-1,point[0])\n",
    "        x=np.linspace(-gaussian_radius,gaussian_radius,(gaussian_radius*2)+1)\n",
    "        gaussian_map=np.multiply(gauss_distribution(x),gauss_distribution(x).T)\n",
    "        gaussian_map/=np.sum(gaussian_map)\n",
    "        \n",
    "        x_left,x_right,y_up,y_down=0,gaussian_map.shape[1],0,gaussian_map.shape[0]\n",
    "        if point[1]<gaussian_radius:\n",
    "            x_left=gaussian_radius-point[1]\n",
    "        if point[0]<gaussian_radius:\n",
    "            y_up=gaussian_radius-point[0]\n",
    "        if point[1]+gaussian_radius>=w:\n",
    "            x_right=gaussian_map.shape[1]-(gaussian_radius+point[1]-w)-1\n",
    "        if point[0]+gaussian_radius>=h:\n",
    "            y_down=gaussian_map.shape[0]-(gaussian_radius+point[0]-h)-1\n",
    "        density_map[\n",
    "            max(0,point[0]-gaussian_radius):min(density_map.shape[0],point[0]+gaussian_radius+1),\n",
    "            max(0,point[1]-gaussian_radius):min(density_map.shape[1],point[1]+gaussian_radius+1),\n",
    "        ]+=gaussian_map[y_up:y_down,x_left:x_rigtht]\n",
    "    density_map/=np.sum(density_map/len(points))\n",
    "    return density_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__ (self, images, maps, batch_size,SUBSAMPLING_FACTOR=8):\n",
    "\n",
    "        self.images = images\n",
    "        self.maps = maps\n",
    "        self.batch_size = batch_size\n",
    "        self.train_image_list=os.listdir(images)\n",
    "        self.SUBSAMPLING_FACTOR=SUBSAMPLING_FACTOR\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.train_image_list)/self.batch_size))\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.__data_generation(idx)\n",
    "        return x,y\n",
    "  \n",
    "    def __data_generation(self, idx):\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        for j in range(idx*self.batch_size, (idx+1)*self.batch_size):\n",
    "            \n",
    "            im_array=img_to_array(load_img(self.images+os.listdir(self.images)[j],target_size=(IN_X,IN_Y)))\n",
    "            im_array/=255.\n",
    "            im_array[:,:,0]=(im_array[:,:,0]-np.mean(im_array[:,:,0]))/np.std(im_array[:,:,0])\n",
    "            im_array[:,:,1]=(im_array[:,:,1]-np.mean(im_array[:,:,1]))/np.std(im_array[:,:,1])\n",
    "            im_array[:,:,2]=(im_array[:,:,2]-np.mean(im_array[:,:,2]))/np.std(im_array[:,:,2])\n",
    "            x.append(im_array)\n",
    "            mat=scipy.io.loadmat(self.maps+os.listdir(self.maps)[j])\n",
    "            points=mat['image_info'][0][0][0][0][0]\n",
    "            points/=self.SUBSAMPLING_FACTOR\n",
    "            \n",
    "            density_map_present=get_density_map_gaussian(im_array,points,sigma=5)\n",
    "            y.append(density_map_present)\n",
    "    return tf.convert_to_tensor(x),tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images='...'\n",
    "train_maps='...'\n",
    "val_images='...'\n",
    "val_maps='...'\n",
    "\n",
    "LR=1e-4\n",
    "BATCH_SIZE=1\n",
    "EPOCH=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(images, maps,BATCH_SIZE,INPUT_DIM)\n",
    "#val_gen = DataGenerator(val_images, val_maps,BATCH_SIZE,INPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b789875",
   "metadata": {},
   "source": [
    "<H1>MODELING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed044d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=(INPUT_DIM,INPUT_DIM,3),\n",
    "        include_top=False,)\n",
    "    \n",
    "    block4_conv3=[base_model.get_layer(layer_name).output for layer_name in [\"block4_conv3\"]]\n",
    "    \n",
    "    return Model(\n",
    "        inputs=[base_model.inputs],outputs=[block4_conv3]\n",
    "    )\n",
    "\n",
    "get_base_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c35420",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f28485",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tf.keras.Input(shape=(IN_X,IN_Y,3))\n",
    "x=get_base_model()(inputs)\n",
    "init=RandomNormal(0.01)\n",
    "\n",
    "x=Conv2D(512, (3,3), activation = 'relu', dilation_rate=2,kernel_iniitalizer=init,padding='same')(x)\n",
    "x=Conv2D(512, (3,3), activation = 'relu', dilation_rate=2,kernel_iniitalizer=init,padding='same')(x)\n",
    "x=Conv2D(512, (3,3), activation = 'relu', dilation_rate=2,kernel_iniitalizer=init,padding='same')(x)\n",
    "x=Conv2D(256, (3,3), activation = 'relu', dilation_rate=2,kernel_iniitalizer=init,padding='same')(x)\n",
    "x=Conv2D(128, (3,3), activation = 'relu', dilation_rate=2,kernel_iniitalizer=init,padding='same')(x)\n",
    "x=Conv2D(64, (3,3), activation = 'relu', dilation_rate=2,kernel_iniitalizer=init,padding='same')(x)\n",
    "x=Conv2D(1, (1,1), activation = 'relu', dilation_rate=1,kernel_iniitalizer=init,padding='same')(x)\n",
    "\n",
    "out=Reshape((96,128))(x)\n",
    "model=tf.keras.Model(inputs=inputs,outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e26903",
   "metadata": {},
   "source": [
    "<H1>TRAINING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = Adam(learning_rate = LR),\n",
    "    metrics='accuracy',\n",
    "    run_eagerly = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8559844",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='...'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac24b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef4899",
   "metadata": {},
   "source": [
    "<h1>TESTING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN ='...'\n",
    "\n",
    "im_array=img_to_array(load_img(train_image+IN_'.jpg',target_size=(IN_X,IN_Y)))\n",
    "im_array/=255.\n",
    "im_array[:,:,0]=(im_array[:,:,0]-np.mean(im_array[:,:,0]))/np.std(im_array[:,:,0])\n",
    "im_array[:,:,1]=(im_array[:,:,1]-np.mean(im_array[:,:,1]))/np.std(im_array[:,:,1])\n",
    "im_array[:,:,2]=(im_array[:,:,2]-np.mean(im_array[:,:,2]))/np.std(im_array[:,:,2])\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imhow(im_array)\n",
    "\n",
    "output=mode.predict(tf.expand_dims(im_array,axis=0))\n",
    "output=np.reshape(output,(OUT_X,OUT_Y))\n",
    "\n",
    "n_people=np.sum(output)\n",
    "mat=scipy.io.loadmat(train_maps+'GT_'+IN+'.mat')\n",
    "points=mat['image_info'][0][0][0][0][0]\n",
    "points/=SUBSAMPLING_FACTOR\n",
    "\n",
    "num_gt=np.squeeze(points).shape[0]\n",
    "print(\"The actual number of people is=\",num_gt)\n",
    "print(\"The predicted number of people is =\",n_people)\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7118c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100b414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

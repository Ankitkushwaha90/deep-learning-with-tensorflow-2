{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import SimpleRNN,Embedding,Input,LSTM,Dense,GRU,Bidirectional,Reshape\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451711be",
   "metadata": {},
   "source": [
    "<H1>DATA PREPARATION</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES=250\n",
    "VALIDATION_RATIO=1\n",
    "VALIDATION_BRIDGE=int(VALIDATION_RATIO*NUM_EXAMPLES)\n",
    "\n",
    "text_dataset=tf.data.TextLineDataset(path).take(NUM_EXAMPLES)\n",
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844da811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(input_text):\n",
    "    return tf.strings.split(input_text,'\\t')[0:1],'starttoken'+tf.strings.split(input_text,'\\t')[1:2],tf.strings.split(input_text,'\\t')[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset=text_dataset.map(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(input_data):\n",
    "    '''\n",
    "    Task: Preprocess sentences or standardize the sentences\n",
    "    Input: raw reviews\n",
    "    output: standardized reviews\n",
    "    '''\n",
    "    output=tf.strings.lower(input_data)\n",
    "    outputs=tf.strings.regex_replace(output,\"<[^>]+>\",\"\")\n",
    "    outputs=tf.strings.regex_replace(output,\"<[%s]\"%re.esceape(string.punctuation),\" \")\n",
    "    outputs=tf.strings.regex_replace(output,\"  \",\" \")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d03098",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=10\n",
    "\n",
    "vectorize_input_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_pre_output_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_output_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:x)\n",
    "vectorize_input_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e21fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:y)\n",
    "vectorize_pre_output_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z:z)\n",
    "vectorize_output_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d72db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_INPUT_SIZE=len(vectorize_input_layer.get_vocabulary())\n",
    "VOCAB_PRE_OUTPUT_SIZE=len(vectorize_pre_output_layer.get_vocabulary())\n",
    "VOCAB_OUTPUT_SIZE=len(vectorize_output_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x,y,z):\n",
    "    return {'in1':tf.squeeze(vectorize_input_layer(x),0),'in2':tf.squeeze(vectorize_pre_output_layer(y),0),}, tf.squeeze(vectorize_output_layer(z),0)\n",
    "dataset=text_dataset.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99833bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_pre_output_layer.get_vocabulary()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d24afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.shuffle(NUM_EXAMPLES)\n",
    "train_dataset=dataset.take(VALIDATION_BRIDGE)\n",
    "validation_dataset=dataset.skip(VALIDATION_BRIDGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ae967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_output_layer.get_vocabulary()[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset=validation_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a325f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21332947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b789875",
   "metadata": {},
   "source": [
    "<H1>MODELING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=8\n",
    "LSTM_ENCODER_HIDDEN_SIZE=300\n",
    "LSTM_DECODER_HIDDEN_SIZE=1000\n",
    "SENTENCE_LENGTH=10\n",
    "\n",
    "inputs=Input(SEQUENCE_LENGTH)\n",
    "pre_out=Input(SEQUENCE_LENGTH)\n",
    "\n",
    "x = Embedding(\n",
    "    VOCAB_INPUT_SIZE,\n",
    "    EMBEDDING_DIM)(inputs)\n",
    "encoder = LSTM(\n",
    "    LSTM_HIDDEN_SIZE,\n",
    "    return_sequences=False,\n",
    "    return_state=True)\n",
    "\n",
    "_,h,c=encoder(x)\n",
    "x=Embedding(VOCAB_OUPUT_SIZE,EMBEDDING_DIM)(pre_out)\n",
    "decoder=LSTM(LSTM_DECODER_HIDDEN_SIZE,\n",
    "            return_state=True,\n",
    "            return_sequences=True)\n",
    "h=Dense(LSTM_DECODER_HIDDEN_SIZE)(h)\n",
    "c=Dense(LSTM_DECODER_HIDDEN_SIZE)(c)\n",
    "\n",
    "x,h,c=decoder(x,[h,c])\n",
    "x=Dense(VOCAB_OUTPUT_SIZE,activation='softmax')(x)\n",
    "model=tf.keras.Model([inputs,pre_out],x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.w_q=tf.keras.layers.Dense(units)\n",
    "        self.w_k=tf.keras.layers.Dense(units)\n",
    "        self.w=tf.keras.layers.Dense(1)\n",
    "    def call(self,query,keys):\n",
    "        values=keys\n",
    "        energy=self.w(tf.nn.tanh(self.w_q(query)+self.w_k(keys)))\n",
    "        attention_weights=tf.nn.softmax(energy,axis=1)\n",
    "        context_vector=attention_weights*values\n",
    "        context_vector=tf.reduce_sum(context_vector,axis=1)\n",
    "        return tf.expand_dims(context_vector,1),attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92539c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,units):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding=Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm=LSTM(units,return_sequences=True)\n",
    "    def call(self,x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.lstm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8570d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98d132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12b4822",
   "metadata": {},
   "source": [
    "<H1>TRAINING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEU(tf.keras.metrics.Metric):\n",
    "    def __init__(self,name='bleu_score'):\n",
    "        super(BLEU,self).__init__()\n",
    "        self.add=0\n",
    "        self.total=0\n",
    "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
    "        y_true=tf.argmax(y_true,-1)\n",
    "        y_pred=tf.argmax(y_pred,-1)\n",
    "        \n",
    "        for i,j in zip(y_pred,y_true):\n",
    "            tf.autograph.experimental.set_loop_options()\n",
    "            self.total+=tf.math.count_nonzero(i)\n",
    "            for word in i:\n",
    "                if word==0:\n",
    "                    break\n",
    "                for q in range(len(j)):\n",
    "                    if j[q]==0:\n",
    "                        break\n",
    "                    if word==j[q]:\n",
    "                        self.add+=1\n",
    "                        j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n",
    "                        break\n",
    "    def result(self):\n",
    "        return self.add/self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "EPOCH=100\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=LR,),\n",
    "    #metrics=[BLEU()],\n",
    "    #run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='...'\n",
    "log_dir='...'\n",
    "callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646752a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35659bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_dataset, validation_data=validation_dataset,verbose=1,epochs=EPOCH,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927a112",
   "metadata": {},
   "source": [
    "<H1>TESTING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=tf.data.Dataset.from_tensor_slices([['i will try']])\n",
    "init_test_data=tf.data.Dataset.from_tensor_slices([['starttoken']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_data=test_data.map(vectorize_input_layer)\n",
    "pre_output_test_data=init_test_data.map(vectorize_pre_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b009cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in input_test_data.take(1):\n",
    "    print(i)\n",
    "    in_1=i\n",
    "for i in pre_output_test_data.take(1):\n",
    "    print(i)\n",
    "    in_2=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(in_1,in_2):\n",
    "    return tf.argmax(model.predict([in_1,in_2]),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ae0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=get_output(in_1,in_2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(SEQUENCE_LENGTH):\n",
    "    print(vectorize_output_layer.get_vocabulary()[output[0][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29368c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107a3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bfffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0694be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0804f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

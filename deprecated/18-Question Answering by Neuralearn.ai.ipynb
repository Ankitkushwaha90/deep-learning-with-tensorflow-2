{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import(SimpleRNN,Embedding,Input,LSTM,Input,\n",
    "                                    Dropout,Dense,GRU,LayerNormalization,\n",
    "                                    Bidirectional,Reshape)\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de4bb7",
   "metadata": {},
   "source": [
    "<h1>Utils</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('...') as f:\n",
    "    data=json.load(f)\n",
    "def get_position(a,b):\n",
    "    b=list(b)\n",
    "    if '' in b:\n",
    "        b.remove('')\n",
    "    j=0\n",
    "    found=0\n",
    "    for i in range(len(a)):\n",
    "        if b[j]==a[i]:\n",
    "            j+=1\n",
    "            if len(b)-1 <j:\n",
    "                found=i\n",
    "                break\n",
    "        else:\n",
    "            j=0\n",
    "    return found-len(b)+1,found\n",
    "\n",
    "f=open('...','a',encoding='utf-8')\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(1):\n",
    "        for k in range(len(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])):\n",
    "            if data[\"data\"][i][\"paragraphs\"][j][\"qas\"][k][\"answers\"]==[]:\n",
    "                pass\n",
    "            else:\n",
    "                init,final=get_position(tf.strings.split(preprocess_sentences(data[\"data\"][i][\"paragraphs\"][j][\"context\"])),\n",
    "                                tf.strings.split(preprocess_sentences(data[\"data\"][i][\"paragraphs\"][j][\"qas\"][k][\"answers\"][0]['text'])))\n",
    "            \n",
    "                f.write(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "                f.write(\"\\t\")\n",
    "                \n",
    "                f.write(data[\"data\"][i][\"paragraphs\"][j][\"qas\"][k][\"question\"])\n",
    "                f.write(\"\\t\")\n",
    "                \n",
    "                f.write(str(init))\n",
    "                f.write(\"\\t\")\n",
    "                \n",
    "                f.write(str(final))\n",
    "                f.write(\"\\n\")\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451711be",
   "metadata": {},
   "source": [
    "<H1>DATA PREPARATION</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('...') as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES=1000\n",
    "VALIDATION_RATIO=1\n",
    "VALIDATION_BRIDGE=int(VALIDATION_RATIO*NUM_EXAMPLES)\n",
    "\n",
    "text_dataset=tf.data.TextLineDataset(path).take(NUM_EXAMPLES)\n",
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844da811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(input_text):\n",
    "    return tf.strings.split(input_text,'\\t')[0:1],tf.strings.split(input_text,'\\t')[1:2],tf.strings.split(input_text,'\\t')[2:3],tf.strings.split(input_text,'\\t')[3:4],tf.strings.split(input_text,'\\t')[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset=text_dataset.map(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(input_data):\n",
    "    '''\n",
    "    Task: Preprocess sentences or standardize the sentences\n",
    "    Input: raw reviews\n",
    "    output: standardized reviews\n",
    "    '''\n",
    "    output=tf.strings.lower(input_data)\n",
    "    outputs=tf.strings.regex_replace(output,\"<[^>]+>\",\"\")\n",
    "    outputs=tf.strings.regex_replace(output,\"<[%s]\"%re.esceape(string.punctuation),\" \")\n",
    "    outputs=tf.strings.regex_replace(output,\"  \",\" \")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d03098",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH=150\n",
    "QUESTION_LENGTH=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ecc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer=TextVectorization(\n",
    "    standardize=preprocess_sentences,\n",
    "    output_sequence_length=CONTEXT_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=text_dataset.map(lambda x,y,z,w,h:h)\n",
    "vectorize_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_INPUT_SIZE=len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c038264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x,y,z,w,h):\n",
    "    return {'in1':tf.squeeze(vectorize_layer(x),0),'in2':tf.squeeze(vectorize_layer(y),0)[0:QUESTION_LENGTH],}, {'out1':tf.cast(tf.squeeze(tf.one_hot(int(z),CONTEXT_LENGTH),0),tf.int32),'out2':tf.cast(tf.squeeze(tf.one_hot(int(w),CONTEXT_LENGTH),0),tf.int32)}\n",
    "dataset=text_dataset.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e21fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in dataset.take(1):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b449d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.shuffle(NUM_EXAMPLES)\n",
    "train_dataset=dataset.take(VALIDATION_BRIDGE)\n",
    "validation_dataset=dataset.skip(VALIDATION_BRIDGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset=validation_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21332947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b789875",
   "metadata": {},
   "source": [
    "<H1>MODELING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.model_size=model_size\n",
    "    def call(self,query,key,value,sequence=None):\n",
    "        score=tf.einsum('ijk,ibk->ijb',query,key)\n",
    "        score/=tf.math.sqrt(tf.cast(self.model_size,tf.float32))\n",
    "        \n",
    "        if sequence is not None:\n",
    "            ones=tf.ones_like(score)\n",
    "            pad_mask=padding_mask(sequence)\n",
    "\n",
    "            total_mask=pad_mask\n",
    "            score+=total_mask*-1e10\n",
    "        alignment=tf.nn.softmax(score,axis=-1)\n",
    "        head=tf.matmul(alignment,value)\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(a):\n",
    "    return tf.expand_dims(tf.cast(tf.math.equal([a],0),tf.float32)[0],axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(model_size):\n",
    "    output=[]\n",
    "    for pos in range(SEQUENCE_LENGTH):\n",
    "        PE=np.zeros((model_size))\n",
    "        for i in range(model_size):\n",
    "            if i%2==0:\n",
    "                PE[i]=np.sin(pos/(10000**(i/model_size)))\n",
    "            else:\n",
    "                PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n",
    "        output.append(tf.expand_dims(PE,axis=0))\n",
    "    return tf.concat(output,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ac783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_size,h):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.query_size=model_size//h\n",
    "        self.key_size=model_size//h\n",
    "        self.value_size=model_size//h\n",
    "        self.h=h\n",
    "        self.dense_q=[Dense(self.query_size) for _ in range(h)]\n",
    "        self.dense_k=[Dense(self.key_size) for _ in range(h)]\n",
    "        self.dense_v=[Dense(self.value_size) for _ in range(h)]\n",
    "        self.dense_o=Dense(model_size)\n",
    "        self.self_attention=SelfAttention(self,key_size)\n",
    "        \n",
    "    def call(self,query,key,value,sequence=None):\n",
    "        heads=[]\n",
    "        \n",
    "        for i in range(self.h):\n",
    "            head=self.self_attention(self.dense_q[i](query),self.dense_k[i](key),\n",
    "                                    self.dense_v[i](value),sequence)\n",
    "            heads.append(head)\n",
    "        heads=tf.concat(heads,axis=2)\n",
    "        heads=self.dense_o(heads)\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(vocab_size,model_size)\n",
    "        self.multi_attention=MultiHeadAttention(model_size,h)\n",
    "        self.dropout=Dropout(0.2)\n",
    "        \n",
    "        self.dense_1=Dense(model_size*4,activation='relu')\n",
    "        self.dense_2=Dense(model_size)\n",
    "        self.feed_forward_norm=LayerNormalization()\n",
    "        \n",
    "    def call(self,enc_in,sequence=None):\n",
    "        enc_out=self.multi_attention(enc_in,enc_in,enc_in,sequence,)\n",
    "        enc_out=enc_in+enc_out\n",
    "        enc_out=self.attention_norm(enc_out)\n",
    "        \n",
    "        feed_forward_in=enc_out\n",
    "        feed_forward_out=self.dropout(self.dense_2(self.dense_1(feed_forward_in)))\n",
    "        feed_forward_out+=feed_forward_in\n",
    "        feed_forward_out=self.feed_forward_norm(feed_forward_out)\n",
    "        return feed_forward_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8559844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h,num_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(vocab_size,model_size)\n",
    "        self.encoder_layer=[EncoderLayer(vocab_size,model_size,h) for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, sequence):\n",
    "        enc_in=self.embedding(sequence)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            out=self.encoder_layer[i](enc_in,sequence)\n",
    "            enc_in=out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac24b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTop(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h,num_layers):\n",
    "        super(EncoderTop,self).__init__()\n",
    "        \n",
    "        self.model_size=model_size\n",
    "        self.num_layers=num_layers\n",
    "        self.h=h\n",
    "        self.embedding=Embedding(vocab_size,model_size)\n",
    "        self.encoder_layer=[EncoderLayer(vocab_size,model_size,h) for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, enc_in):\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            out=self.encoder_layer[i](enc_in,None)\n",
    "            enc_in=out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,model_size,h,num_layers):\n",
    "        super(CQAttention,self).__init__()\n",
    "        \n",
    "        \n",
    "    def call(self, enc_in):\n",
    "        score=tf.einsum('BNC,BCD->BND',context,query)\n",
    "        score=tf.nn.softmax(score,axis=-2)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE=256\n",
    "NUM_HEADS=8\n",
    "NUM_LAYERS=3\n",
    "\n",
    "in_1=Input(CONTEXT_LENGTH)\n",
    "in_1=Input(QUESTION_LENGTH)\n",
    "\n",
    "enc_1_out=Encoder(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(in_1)\n",
    "enc_2_out=Encoder(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(in_2)\n",
    "\n",
    "out=CQAttention()(enc_1_out,enc_2_out)\n",
    "\n",
    "out_1=EncoderTop(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(out)\n",
    "out_2=EncoderTop(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(out_1)\n",
    "out_3=EncoderTop(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(out_2)\n",
    "out_4=EncoderTop(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(out_3)\n",
    "\n",
    "head_start=tf.concat([out_1,out_2,out_3],axis=-1)\n",
    "head_end=tf.concat([out_1,out_4],axis=-1)\n",
    "\n",
    "head_start=tf.nn.softmax(Dense(1)(head_start),axis=-2)\n",
    "head_end=tf.nn.softmax(Dense(1)(head_end),axis=-2)\n",
    "\n",
    "out_4=EncoderTop(VOCAB_INPUT_SIZE,MODEL_SIZE,NUM_HEADS,NUM_LAYERS)(out_3)\n",
    "\n",
    "out={\"out1\":tf.squeeze(head_start,axis=-1),\"out2\":tf.squeeze(head_end,-1)}\n",
    "model=tf.keras.Model([in_1,in_2],out,name='CQANet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0192b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12b4822",
   "metadata": {},
   "source": [
    "<H1>TRAINING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true,y_pred):\n",
    "    precision=tf.keras.metrics.Precision()(y_true,y_pred)\n",
    "    recall=tf.keras.metrics.Recall()(y_true,y_pred)\n",
    "    return 2*precision*recall/(precision+recall+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1e-4\n",
    "EPOCH=1000\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=LR,),\n",
    "    metrics=f1_score,\n",
    "    run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='...'\n",
    "log_dir='...'\n",
    "callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35659bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_dataset,verbose=1,epochs=EPOCH,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927a112",
   "metadata": {},
   "source": [
    "<H1>TESTING</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"...\"\n",
    "question=\"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1=tf.data.Dataset.from_tensor_slices([[context]])\n",
    "test_data_2=tf.data.Dataset.from_tensor_slices([[question]])\n",
    "\n",
    "input_test_data_1=test_data_1.map(vectorize_layer)\n",
    "input_test_data_2=test_data_2.map(vectorize_layer)\n",
    "\n",
    "for i in input_test_data_1.take(1):\n",
    "    in_1=i\n",
    "for i in input_test_data_2.take(1):\n",
    "    in_2=i[:,0:QUESTION_LENGTH]\n",
    "    \n",
    "start=tf.argmax(model.predict([in_1,in_2])['out1'],-1)[0].numpy()\n",
    "end=tf.argmax(model.predict([in_1,in_2])['out2'],-1)[0].numpy()\n",
    "\n",
    "out=tf.strings.split(preprocess_sentences(context),' ')[start:end+1]\n",
    "\n",
    "answer=\"\"\n",
    "for i in out:\n",
    "    answer+=i\n",
    "    answer+=\" \"\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b009cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257e382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9761628d",
   "metadata": {},
   "source": [
    "<h1>LSHAttention</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_one_back(x):\n",
    "    x_extra=tf.concat([x[:,-1:,...],x[:,:-1,...]],axis=1)\n",
    "    return tf.concat([x,x_extra],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sticker_look_one_back(x):\n",
    "    x_extra=tf.concat([x[:-1:],x[:,:-1]],axis=1)\n",
    "    return tf.concat([x,x_extra],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_masker(a,b):\n",
    "    a,b=tf.cast(a,dtype=tf.float32)+0.01,tf.cast(b,dtype=tf.float32)+0.01\n",
    "    vals=tf.einsum('ipj,ipk->ipjk',b,1/a)\n",
    "    out=tf.cast(tf.cast(tf.cast(vals,dtype=tf.int32),dtype=tf.bool),dtype=tf.int32)\n",
    "    out=-out+1\n",
    "    return tf.cast(out,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,bucket_size=8,n_hashes=1):\n",
    "        super(LSHAttention,self).__init__()\n",
    "        self.n_hashes=n_hashes\n",
    "        self.bucket_size=bucket_size\n",
    "        \n",
    "    def call(self,query,key,value,causal_masking=False):\n",
    "        R=tf.random.normal((tf.shape(query)[0],tf.shape(query)[-1],self.bucket_size//2))\n",
    "        xR=tf.matmul(query,R)\n",
    "        concat_xR=tf.concat([xR,-xR],axis=-1)\n",
    "        buckets=tf.math.argmax(concat_xR,axis=-1)\n",
    "        \n",
    "        sticker=tf.argsort(buckets)\n",
    "        undo_sort=tf.argsort(sticker)\n",
    "        sorted_query=tf.gather(query,sticker,axis=1,batch_dims=1)\n",
    "        sorted_value=tf.gather(value,sticker,axis=1,batch_dims=1)\n",
    "        \n",
    "        chunked_query=tf.stack(tf.split(sorted_query,self.bucket_size,1),1)\n",
    "        chunked_value=tf.stack(tf.split(sorted_value,self.bucket_size,1),1)\n",
    "        \n",
    "        sticker=tf.stack(tf.split(sticker,self.bucket_size,1),1)\n",
    "        new_sticker=sticker_look_ne_back(sticker)\n",
    "        \n",
    "        lb_chunked_query=look_one_back(chunked_query)\n",
    "        lb_chunked_value=look_one_back(chunked_value)\n",
    "        \n",
    "        score=tf.einsum('bhie,bhje->bhij',chunked_query,lb_chunked_query)\n",
    "        score/=tf.math.sqrt(tf.cast(query.shape[-1],tf.float32))\n",
    "        \n",
    "        if causal_masking==True:\n",
    "            causal_mask=causal_masker(sticker,new_sticker)\n",
    "            dots+=causal_mask*-1e-10\n",
    "        score=tf.nn.softmax(score)\n",
    "        output=tf.einsum('buij,buje->buie',score,lb_chunked_value)\n",
    "        \n",
    "        sorted_output=tf.reshape(output,(tf.shape(output)[0],tf.shape(query)[i],output.shape[3]))\n",
    "        output=tf.gather(sorted_output,undo_sort,axis=1,batch_dims=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29368c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107a3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bfffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0694be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0804f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
